# A Clustering-Based Backdoor Defense Method for Federated Learning with the Adam Optimizer
## Abstract
Federated learning is a distributed machine learning paradigm that enables multiple clients to collaboratively train a global model without sharing local data. Due to its distributed nature and the inability to access local datasets, federated learning is particularly vulnerable to backdoor attacks. Meanwhile, the heterogeneity of distributed data further complicates the detection of such attacks. However, existing defense strategies often overlook the presence of non-stationary objectives and noisy gradients across multiple clients, making it challenging to accurately and efficiently identify malicious participants. To address these challenges, we propose a novel and efficient federated learning defense algorithm leveraging the Adam optimizer, incorporating both coarse-grained and fine-grained clustering mechanisms to neutralize backdoor attacks. First, the Adam optimizer accelerates the learning process by mitigating the impact of noisy gradients and addressing the non-stationary objectives posed by different clients under attack. Second, a coarse-grained clustering method based on the minimum spanning tree and a fine-grained clustering process is considered together to differentiate between benign clients and potential attackers. This is followed by an adaptive clipping strategy to further alleviate the influence of malicious attackers. Our theoretical analysis demonstrates the consistent convergence of Adam in a federated backdoor defense environment. Extensive experimental results validate the effectiveness of our approach, showing that it outperforms state-of-the-art baselines in identifying and mitigating the impact of backdoor attacks.
