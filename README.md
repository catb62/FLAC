# FLAC: A Clustering-Based Backdoor Defense Method for Federated Learning with the Adam Optimizer
Federated learning is highly vulnerable to security threats, particularly federated backdoor attacks, due to its distributed and heterogeneous (Non-IID) settings. Effective backdoor defense mechanisms are crucial to preserving the integrity of federated systems. However, existing defense strategies often overlook the presence of the non-stationary objectives and noisy gradients across multiple clients, making it challenging to accurately and efficiently identify malicious participants. To address these challenges, we propose a novel and efficient federated learning defense algorithm leveraging the Adam optimizer, incorporating both coarse-grained and fine-grained clustering mechanisms to neutralize backdoor attacks. First, the Adam optimizer accelerates the learning process by mitigating the impact of noisy gradients and addressing the non-stationary objectives posed by different clients under attack. Second, a coarse-grained clustering method based on the minimum spanning tree and a fine-grained clustering process are considered together to differentiate between benign clients and potential attackers. This is followed by an adaptive clipping strategy to further isolate and eliminate attackers. Our theoretical analysis demonstrates the consistent convergence of Adam in a federated backdoor defense environment. Extensive experimental results validate the effectiveness of our approach, showing that it outperforms state-of-the-art baselines in identifying and mitigating backdoor attacks.
